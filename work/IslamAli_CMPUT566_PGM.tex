\documentclass{article}
\usepackage[margin=1in]{geometry}  % set the margins to 1in on all sides
\usepackage{graphicx}              % to include figures
\usepackage{epstopdf}
\usepackage{amsmath}               % great math stuff
\usepackage{amsfonts}              % for blackboard bold, etc
\usepackage{amsthm}                % better theorem environments
\usepackage{amssymb}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{lipsum,array,amsmath}
\usetikzlibrary{positioning,automata}
\usetikzlibrary{arrows.meta}
\usepackage{pgfplots}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{algorithm,algorithmic}
\usepackage{bbold}
\usepackage{tabularx}
\usepackage{mathtools}


\pagestyle{fancy}
\fancyhf{}
\rhead{Islam A. Ali}
\lhead{CMPUT-566: Probabilistic Graphical Models}
\rfoot{Page \thepage}
\lfoot{\scriptsize{}}


\begin{document}


% =================== Header ====================
\begin{center}
{\bf CMPUT 566: Introduction to Machine Learning}  \\
{A Self-Study Report on: Probabilistic Graphical Models (PGMs)} \\
\vspace{.1in}
{\em Islam A. Ali}\\
{\em Student ID: 1633813}\\
{\em iaali@ualberta.ca}\\
\end{center}
\noindent\rule{\textwidth}{1pt}
% ==========================================================================

\section*{Abstract}
Due to the success achieved by machine learning algorithms in a wide range of applications, the need to have models that can accommodate complex tasks and being able to infer results in an efficient way became a necessity. These complex tasks come equipped with an extended number of variables and a complex network of relations that can results in inefficient modeling in case of using conventional learning and inference models. Probabilistic Graphical Models (PGMs) represents an efficient method for dealing with such complex tasks by combining concepts from probability theory with graph theory. They depend on modeling random variables and the dependencies among them in the form of either a directional acyclic graph (in case of Bayesian Networks) or as a indirected graphs (in case of Markov Networks). The main advantage of such models is the utilization of the conditional independence property that results in an efficient structure for inference. In this report, we present a summary of PGMs and their usage in machine learning tasks with a focus on the Bayesian Networks. The report starts by giving a brief intro about the background needed for this topic. Then, The representation of PGMs, specially Bayesian Networks is presented in detail. Following that, the concepts of learning in Bayesian Networks as well as inference is discussed. The report concluded by mentioning some of the application of PGMs such as expert systems, and provided a brief discussion PGMs pros and cons.  

\noindent\rule{\textwidth}{1pt}
% ==========================================================================
% Report Body
\section{What are Probabilistic Graphical Models (PGMs)?}
Probabilistic Graphical Models (PGMs) are statistical model that can be used to represent complex dependencies between random variables efficiently using graphs data structure. This dependencies are modeled as joint distributions. However, with the increase of the number of random variables involved, the need to have a more efficient way of representation is needed. The graph representation provide efficiency in terms of time complexity as well as scale-ability to more complex systems as it models random variables and the conditional dependencies among them allowing for scale-ability in terms of problems studied.
\section{Motivation}
To motivate the idea of having a more complexity-efficient statistical model for dependencies representation, an example of industrial defects in products diagnosis system is discussed. Let's say a product in a production line can either be malfunctioned electrically or mechanically or both. This can be represented by a joint distribution of two random variables where each is representing a certain malfunction. Further, we can expand the diagnosis system to the reason for defect. For instance in the electrical defects, it is because of PCBs manufacturing or due to components defects, on the other hand, the mechanical malfunction can be either from the material used or the methodology utilized. And the relations goes on, with extended number of random variables, that can be very expensive to represent or to query using regular mathematical methods. In this case, the rise of the PGMs makes sense, where such a system can be represented as a graph with nodes representing each random variable in the system, and with directed edges representing the dependencies and their directions. \\
\indent Other motivation comes from the graph theory itself, where a wide range of research was done and was proven to be correct for data dependency representation, query, and manipulation. A wide range of efficient algorithms are available for all these purposes and even beyond.
% ==========================================================================
\section{Mathematical Background}
In this section, we briefly review the main concepts that will be used throughout this report. For this purpose, a review of both probability theory and the graph theory are reviewed briefly by mentioning their most famous/important rules and concepts. This quick review intentions is to act as a reminder and not an extensive review of theories. 
\subsection{Probability Theory}
Probability theory's main objective is to systematically study uncertainty or the degree of confidence in quantities or measurements. The following are the three main axioms of probability:
\begin{enumerate}
 \item \textbf{Sample Space:} It is the set of all possible outcomes of a certain experiments, it is denoted by $\Omega$.
 \item \textbf{Event Space:} It is a subset of the sample space and represents a limited set of possible outcomes of the experiment, it is denoted by $\digamma$.
 \item \textbf{Probability Measure:} It is a measure that maps the event space to real numbers, it is denoted by $P$, and it satisfies a number of rules:
 	\begin{itemize}
 	\item $P(A) \geq 0 $
 	\item $P(\Omega) = 1$
 	\item The union of disjoint events is the summation of their probabilities.
 	\end{itemize}
\end{enumerate} 

\subsubsection{Conditional Probability and Independence}
The conditional probability is the probability of the occurrence of an event after observing another event. This is given by the following formula:
\begin{equation}
P(A|B) = \frac{P(A \cap B)}{P(B)}
\end{equation}
Independence, on the other hand, is mainly defined as the first event having no impact on the second event and can be mathematically represented as:
\begin{equation}
P(A \cap B) = P(A)P(B)
\end{equation}

\subsubsection{Results of Conditional Probability: Chain Rule and Bayes Rule}
Based on the conditional probability, we can deduce the following relation that is called the chain rule:
\begin{equation}
P(A \cap B) = P(A|B) P(B)
\end{equation}
More generally, if a series of events $A_1, A_2, ..., A_n$, the probability of a certain event can be given by:
\begin{equation}
P(A_1 \cap ... \cap A_n) = P(A_1)P(A_1|A_2) ... P(A_1|A_2 \cap ... \cap A_n)
\end{equation}
Another result of the conditional probability, is the Bayes Rule which allows to compute a certain conditional probability from its inverse with some knowledge of probability priors. The mathematical representation is defined as:
\begin{equation}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{equation}

\subsubsection{Random Variables, Marginal, and Joint Distributions}
A random variable is defined by a function that assigns a value for all possible outcome of a certain measurable experiment. It is a very useful when dealing with complex and dependent relations of events where each has its own probabilistic properties. \\ 

One of the associated concepts to random variables is marginal distribution. The marginal distribution of random variable $X$ is defined as the distribution over the events in which $X$ is involved, and is denoted by $P(X)$. Another concept associated with random variable is joint distributions, which is defined as the distribution over events in which two or more random variables are involved, and is denoted by $P(X,Y)$. It can also be mathematically represented by:
\begin{equation}
P_{XY}(x,y) = P_{XY}(y|x)P_Y(x)
\end{equation}

\subsubsection{Mean/Expectation and Variance}
Two other concepts related to probability theory are mean and variance which can be used to describe a probability distribution. The mathematical formula for the expectation in both the discrete and continuous cases are given by:
\begin{equation}
\mathbb{E} [X] = \sum_x x.P(x)
\end{equation}
\begin{equation}
\mathbb{E} [X] = \int_x x.P(x) \; dx
\end{equation}
While the variance is defined as: 
\begin{equation}
\mathbb{V}ar [X] = \mathbb{E} [X^2] - \mathbb{E}^2 [X]
\end{equation}
% ==========================================================================
\subsection{Graph Theory}
A graph is a data structure that is structured from two elements that are: nodes and edges. It is mainly used to represented relations between generic data elements via connections among nodes via edges. Graphs can have multiple structure and can be categorized based on many aspects such as:
\begin{enumerate}
\item What nodes represent?
\item Is it a directed or undirected graph?
\item Is it a fully or partially directed graph?
\item Are the edges weighted or not?
\item Are cycles and loops allowed or not?
\item What are the traversal techniques allowed?
\end{enumerate}
In the context of this report, nodes represent random variables while edges represent the conditional relationship between random variables. Edges in this context can either be directed or undirected, and can allow loops and cycles. The following is an example of a directed graph \footnote{Image was downloaded from: \url{https://computersciencewiki.org/index.php/The_web_as_a_directed_grap}}:
\begin{figure}[H]
\centering
\includegraphics[width=0.25\textwidth]{Directed_graph.png}
\caption{Example of Directed Graph }
\end{figure}
% ==========================================================================

\section{PGMs Representation}
PGMs has many representations such as \textit{Bayesian Networks} where edges are directed and are indicated by an arrow in the graphical representation. Another type is the \textit{Markov Random Fields} in which edges are undirected and are represented by lines. The first method is useful for conditional dependency representation, while later one is useful in representing soft constraints. In this report, we focus on directed probabilistic graphical models, known as \textbf{"Bayesian Networks"}.
\subsection{Bayesian Networks Representation}
Bayesian Networks, a.k.a Bayesian Belief Networks (BBNs), are PGMs that represent the conditional relations between random variables by directed acyclic graph (DAG). \\
Formally, a Bayesian Network $BN$ is represented by:
\begin{equation}
BN = (g, \{ P{X_1},..., P{X_N}  \})
\end{equation}
\textbf{Where:} $g$ is a directed acyclic graph (DAG) represented as $g=(X,E)$. From the definition of the DAG, $X$ are the graph nodes representing random variables, while $E$ are the edges connecting nodes to each other representing conditional independence between random variables. $P{X_1},..., P{X_N}$ are the conditional probability distributions associated with each node/random variable. It is represented as a table indicating the relations between ancestors of a certain node and the resulting probability for each outcome of the same node. In the following diagrams we present the probability distribution table associated with a certain node in the BN context.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{cond_prob.png}
	\caption{BN with Conditional Probability Table}
\end{figure}
For the sake of formulation, we also define the parameter $\Theta = \{\theta_1, ..., \theta_k\}$, which are the parameters which define the probability distribution for each node. For instance, in case of a Gaussian distribution, for the node $X_i$, the set of parameters defining it are given by: 
\begin{equation}
\Theta_i = (\mu_i, \sigma_i)
\end{equation}
\textbf{Where:} $\mu_i$ and $\sigma_i^2$ are the mean and variance of the Gaussian distribution describing node $X_i$. Accordingly, the BN can be defined in terms of the parameters defining the probability distributions as follows:
\begin{equation}
BN = (g, \{ \Theta_1,..., \Theta_N  \})
\end{equation}
It worth mentioning that in some of the references, the parameter $\Theta$ refers to the weights and bias for each probability distribution rather than the mean and variance or the parameters defining the distribution. The definition of weight and bias and their impact on the distributions are not provided, unless they refer to the weight and bias governing the mean definition, which will then make $\Theta=(w_i, b_i, \sigma_i)$. 

\subsubsection{Factorization Properties}
The factorization property allow for defining the joint distribution as a multiplication of conditional probabilities governed by the conditional independence implied in the graph structure. Each of the conditional probability distributions are denoted by:
\begin{equation}
P_{X_i} = (X_i | P_g (Xi))
\end{equation}
Which means that the probability of a certain random variable is defined by the conditional probability relative to its parents/ancestors in the DAG. \\
Based on the previous relation and the definition of a joint probability given in the introduction, the complete joint probability of the BN $P_{BN}(X_i, ..., X_N)$ (Where N is the number of random variable in the BN), is given by:
\begin{equation}
P_{BN}(X_i, ..., X_N) = \prod_{i=1}^{N} (X_i | {Parent}_g (Xi))
\end{equation}
Applying this rule on the example BN given in Fig. 2, we can end up with the following joint distribution (note that for each node, the initials are used as random variable names. e.g. Storm --> S).
\begin{equation}
P_{BN}(X_i, ..., X_N) = P(S).P(B).P(L|S).P(C|S,B).P(T|L).P(F|L,S,C)
\end{equation}
Based on the joint distribution factorization, one can easily proof that conditional probability in case of parameters independence. For this, we seek to get the conditional probability of a certain node given all other node available in the graph. For example, we seek to get $P(C|S,B, L, T, F)$ in the previously mentioned example in Fig. 2. According to the relation between conditional probability and the factorization principle illustrated, the conditional probability can be re-written as:
\begin{equation}
P(C|S,B, L, T, F) = \frac{P(C,S,B,L,T,F)}{P(S,B,L,T,F)}
\end{equation}
The joint distributions can be further expanded as:
\begin{equation}
P(C,S,B,L,T,F) = P(S)P(B)P(L|S)P(C|S,B)P(T|L)P(F|L,S,C)
\end{equation}
\begin{equation}
P(S,B,L,T,F) = \sum_{C} P(C,S,B,L,T,F)
\end{equation}
\begin{equation}
P(S,B,L,T,F) = \sum_{C} P(S)P(B)P(L|S)P(C|S,B)P(T|L)P(F|L,S,C)
\end{equation}
\begin{equation}
P(S,B,L,T,F) = P(S)P(B)P(L|S)(\sum_{C}P(C|S,B))P(T|L)P(F|L,S,C)
\end{equation}
The summation over all values of a random variable =1, from the basic probability axioms, which imply:
\begin{equation}
P(S,B,L,T,F) = P(S)P(B)P(L|S)P(T|L)P(F|L,S,C)
\end{equation}
Therefore, the conditional probability can be re-written as:
\begin{equation}
P(C|S,B, L, T, F) = \frac{P(S)P(B)P(L|S)P(C|S,B)P(T|L)P(F|L,S,C)}{P(S)P(B)P(L|S)P(T|L)P(F|L,S,C)}
\end{equation}
After canceling out equal terms in the fraction, we end up with:
\begin{equation}
P(C|S,B, L, T, F) = P(C|S,B)
\end{equation}
Which imply the independence of the node of any non-parent node. 

\subsubsection{Conditional Independence Property}
Conditional independence is a very important property when dealing with BNs that can substantially simplify the structure of the joint distributions and can provide a concise presentation of it when factorized. As suggested by the literature, conditional independence is considered the backbone of BNs due to the face that the structure of the graph and the complexity reduction advantage are mainly basing on this property. \\
Consider the case when we have a BN with three nodes, where nodes c->a and c->b are connected. The independence of random variable $a$ and $b$ are coming from the definition of the c and its relation to both random variables. The following is the definition of conditional independence:
\begin{equation}
P(a,b|c) = P(a|c)P(b|C)
\end{equation}
In this case, $a$ and $b$ are declared conditionally independent given $c$ and this can be denoted by the perpendicular sign $\perp$ such that:
\begin{equation}
P(a \perp b |c) = P(a|c)
\end{equation}
\begin{equation}
P(b \perp a |c) = P(b|c)
\end{equation}
This definition stems from the definition of the joint distribution of independent random variables given by (in case of $a$ and $b$ begin independent):
\begin{equation}
P(a,b) = P(a)P(b)
\end{equation} 
Alternatively, the definition of conditional independence can be given by:
\begin{equation}
P(a|b,c) = P(a|c)
\end{equation}
The proof for this relation comes from the joint distribution definition and its relation to the conditional probability definition, and it goes as follows:
\begin{equation}
P(a|b,c) = \frac{P(a,b,c)}{P(b,c)}
\end{equation}
\begin{equation}
P(a|b,c) = \frac{P(a,b|c)P(c)}{P(b|c)P(c)}
\end{equation}
From the first definition of conditional independence:
\begin{equation}
P(a|b,c) = \frac{P(a|c)P(b|c) P(c)}{P(b|c)P(c)}
\end{equation}
Canceling out equivalent terms:
\begin{equation}
P(a|b,c) = P(a|c)
\end{equation}
To show the impact of the conditional independence on the complexity reduction, we consider the following network structure. Assume we have $n=20$ binary random variables, with a structure having each node in the network to have a maximum of 4 parent nodes. The number of probability values needed to be calculated in order to decide on an outcome in case of having no conditional independence assumption is given by:
\begin{equation}
count(P)_{total} = 2^{n} = 2^{20}
\end{equation}
While in case of utilizing the reduction from the conditional independence the number goes down to:
\begin{equation}
count(P)_{total} = n*(2^k) = 20*(2^4)= 20*16= 320 \approxeq 2^9
\end{equation}
Which is a huge improvement in terms of complexity.
\subsubsection{Proof for The Conditional Independence Property}
In order to prove the conditional independence property, we define three types of nodes with respect to a certain $X_i$ node:
\begin{enumerate}
\item Descendant Node: Which is one of the children of $X_i$, either directly or non-directly.
\item Parent Node: Which are nodes having $X_i$ as one of their descendants.
\item Non Descendant Node: Which are node that are neither descendants nor parents.
\end{enumerate}
Intuitively, a node have dependency on descent node, then the following relation is sufficient to prove the conditional dependency:
\begin{equation}
P(X_i|non-desc(X_i),Parent(X_i)) = P(X_i|Parent(X_i)) 
\end{equation} 
Again, using the joint probability and the relation to conditional probability, we get:
\begin{equation}
P(X_i|non-desc(X_i),Parent(X_i)) = \frac{P(X_i, non-desc(X_i), Parent(X_i))}{P(non-desc(X_i)P(Parent(Xi)))}
\end{equation}
\begin{equation}
numerator = \sum_{desc(X_i)} P(X_i, non-desc(X_i), Parent(X_i), desc(X_i))
\end{equation}
Which can be factorized based on each node type as follows:
\begin{equation}
= \sum_{desc(X_i)} (P(X_i|Parent(X_i)))*(\prod_{\mathclap{X_j \in desc(X_i)}} P(X_j|Parent(X_j)))*(\prod_{\mathclap{X_k \in {non-desc(X_i) \bigcup parent(X_i)}}} P(X_k|Parent(X_k)))
\end{equation}
Given the fact that the summation over all values of desc = 1 from the prob. axioms, the numerator can be reduced to:
\begin{equation}
= (P(X_i|Parent(X_i))) \sum_{desc(X_i)}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(\prod_{\mathclap{X_k \in {non-desc(X_i) \bigcup parent(X_i)}}} P(X_k|Parent(X_k)))
\end{equation}
Processing the denominator will be as follows:
\begin{equation}
denominator = \sum_{desc(X_i)}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\ (\prod_{\mathclap{X_k \in {non-desc(X_i) \bigcup parent(X_i)}}} P(X_k|Parent(X_k)))
\end{equation}  
by dividing the above two equations, we get the final formula as:
\begin{equation}
P(X_i|non-desc(X_i),Parent(X_i)) = P(X_i|Parent(X_i)) 
\end{equation} 
Which proves the conditional independence property.
% ==========================================================================
\section{Learning in PGMs}
After illustrating the representation of BNs, and the significance of their usage in machine learning problems, the learning process of this model is now discussed. The objective of the learning process it to have a BN that we can use to get the probability distribution of a random variable over all possible values of this variable. In this context we do have three cases as follows:
\begin{enumerate}
\item Both BN structure and conditional probabilities are known, and in this case, the inference process is straight forward. 
\item Only the structure of the BN is known, and the conditional probabilities are not defined. In this case, The learning process would be to define the probability distribution tables associated with each random variable. In this stated that, domain experts may define the optimal structure of the BN but the training data may not have enough observability over internal random variables.
\item Both the structure and the conditional probabilities are not known and the objective of the training process is to determine both the optimal structure and the conditional distributions given the training data. 
\end{enumerate}
In all cases, the learning process will depend on the the specific task and the available knowledge of the system and whether the domain experts are available for providing the optimal structure of the BN. Additionally, it worth mentioning that having a full automated model that can learn both the structure of the network and the probability distributions and the conditional relations is the optimal case and the ultimate objective in learning PGMs.
\subsection{Learning The Conditional Distribution Tables of BNs}
In the case where the structure of the network is defined, meaning that for each random variable/node in the BN $X_i$; the parents $Parent(X_i)$ and descendants $Desc(X_i)$ of this random variables $X_i$ are defined and the conditional independences are known. In this case, we assume partial observability of conditional distribution tables associated with each node. In this section, we discuss the Gradient Ascent Algorithm that can be used to learn the entries of the tables given the structure and partial observability. From an abstract point of view, the algorithm depends on searching in hypotheses $H$ and tries to maximize $P(D|h)$, where $D$ is the available training data, and $h$ is one hypothesis in the hypothesis space. More formally, we need to define the following terms:
\begin{itemize}
\item $P(D|h)$: is the probability of the training data given a certain hypothesis. It is also denoted by $P_h (D)$ for simplicity.
\item $d \in D$: is a data sample inside the training data, the training data $D$ is of size $M$.
\item $w_{ijk}$: is an entry in one of the conditional distribution tables associated with each random variable $Y_i$.
\item $Y_{i}$: is the random variable in hand to which the table is being calculated.
\item $U_{i}$: is the set of parent nodes for the random variable $Y_i$.
\item $y_{ij}$: is one value of possible values for random variable $Y_i$.
\item $u_{ik}$: is one value of possible values for parent $U_k$ or random variable $Y_i$.
\end{itemize}
The objective is to maximize $P(D|h)$ for each of the tables entries for each node in the network. The maximization is achieved by calculating the gradient at each $w_ijk$ for the $lnP_h(D)$. The objective is to proof the following relation:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{P(Y_i = y_{ij}, U_i=u_{ik} | d)}{w_{ijk}}
\end{equation}
Assuming the data $d \in D $ is $iid$:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \frac{\partial}{w_{ijk}} ln \prod_{d \in D} P_h(d)
\end{equation}
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{\partial ln P_h(D)}{\partial w_{ijk}}
\end{equation}
The partial derivative of $ln$ is given by:
\begin{equation}
\frac{\partial ln f(x)}{\partial x} = \frac{1}{f(x)}. \frac{\partial f(x)}{\partial x}
\end{equation}
Then, the equation will be:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{1}{P_h(D)} . \frac{\partial P_h(D)}{\partial w_{ijk}}
\end{equation}
In order to determine the gradient, $P(D|h)$ needs to be represented using $y_{ij}$ and $u_{ik}$ as follows:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{1}{P_h(D)} . \frac{\partial}{\partial w_{ijk}} \sum_{j^{'},k^{'}} P_h(d|y_{ij^{'}}, u_{ik^{'}})P_h(y_{ij^{'}}, u_{ik^{'}})
\end{equation}
Following the product rule, we can further expand the equation as follows:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{1}{P_h(D)} . \frac{\partial}{\partial w_{ijk}} \sum_{j^{'},k^{'}} P_h(d|y_{ij^{'}}, u_{ik^{'}})P_h(y_{ij^{'}}| u_{ik^{'}})P_h(u_{ik^{'}})
\end{equation}
The summation is done over $j^{'}$ and $k^{'}$ as they represent all entries in the network, however, for the value to be non-zero, the following conditions must hold:
\begin{equation}
j^{'} = j \;\;\;\;\;\; k^{'}=k
\end{equation}
Using this constraint over the equation, the equation can be reduced to the following form:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{1}{P_h(D)} . \frac{\partial}{\partial w_{ijk}} P_h(d|y_{ij}, u_{ik})P_h(y_{ij}| u_{ik})P_h(u_{ik})
\end{equation}
The entry $w_{ijk}$ is essentially the prob. of the random variable given its parents, and can be re-written as:
\begin{equation}
w_{ijk} = P_h(y_{ij}| u_{ik})
\end{equation}
Therefore, the gradient can be expressed as:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{1}{P_h(D)} . \frac{\partial}{\partial w_{ijk}} P_h(d|y_{ij}, u_{ik}) w_{ijk} P_h(u_{ik})
\end{equation}
And the result of the partial derivative will result in excluding this term as:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{1}{P_h(D)} . P_h(d|y_{ij}, u_{ik}) P_h(u_{ik})
\end{equation}
The conditional prob. $P_h(d|y_{ij}, u_{ik})$ can be re-written by the Bayes rule as: 
\begin{equation}
P_h(d|y_{ij}, u_{ik}) = \frac{P_h(y_{ij}, u_{ik} |d) P_h(d)}{P_h(y_{ij}, u_{ik})}
\end{equation}
Plugging this relation into the gradient equation, we end up with the following relation:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{1}{P_h(D)} \frac{P_h(y_{ij}, u_{ik} |d) P_h(d)}{P_h(y_{ij}, u_{ik})} P_h(u_{ik})
\end{equation}
Canceling out $P_h(D)$:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{P_h(y_{ij}, u_{ik} |d)}{P_h(y_{ij}, u_{ik})} P_h(u_{ik})
\end{equation}
Using the relation between the joint probability and the conditional prob. we can further reduce the equation into:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{P_h(y_{ij}, u_{ik} |d)}{P_h(y_{ij}| u_{ik})}
\end{equation}
Using the definition of $w_{ijk} = P_h(y_{ij}| u_{ik})$, we can re-write the gradient to be:
\begin{equation}
\frac{\partial ln P(D|h)}{\partial w_{ijk}} = \sum_{d \in D} \frac{P_h(y_{ij}, u_{ik} |d)}{w_{ijk}}
\end{equation}
Which is the objective we started trying to prove. \\

\subsubsection{Gradient Ascent Algorithm}
For the learning to happen, we can use a gradient ascent algorithm that can be formed as:
\begin{equation}
w_{ijk} = w_{ijk} + \alpha \sum_{d \in D} \frac{P_h(y_{ij}, u_{ik} |d)}{w_{ijk}}
\end{equation}
Where $\alpha$ is the learning rate and is considered a hyperparameter. The main constraint that must be respected is that the values will need to be normalized in order to have all conditional probabilities to $=1$ for a certain given random variable $Y_i$, that's to satisfy the probabilities axioms mentioned earlier in this report.

\subsection{Learning the Structure of the BN}
The problem of deducing the structure of the BN as well as the conditional probability tables associated with it, is considered a PN-Hard problem due to the possible configurations of networks given a certain dataset. One solution to this problem an algorithm called K2 algorithm, which is essentially doing heuristic search over alternative forms of the BN, but the algorithm assumes full observability of the training data and the relations. Other algorithms are also available, and they differ in terms of the needed observability in the training data and the trade-off made between accuracy and efficiency. Another aspect to look after in the context of learning the BN structure is the scoring function or the measure by which a certain structure can be judged compared to others.
% ==========================================================================
\section{Inference in PGMs}
Inference is the process of inferring the probability distribution of a certain query variable given some observable variables in the system. The assumption here is that the structure of the network as well as the values of the conditional probability tables are all defined either before hand in the experiment setup or by learning as illustrated in the previous section. In this context we define the following sets:
\begin{itemize}
\item $O$: The set of observed random variables.
\item $Q$: The set of query random variables.
\item $H$: The set of random variables that are not one of the above.
\end{itemize}
Together they construct the set of random variable in BN such that:
\begin{equation}
X = O \cup Q \cup H
\end{equation}
Another condition on these sets is that they do not intersect with each other:
\begin{equation}
O \cap Q = Q \cap H = O \cap H = \O
\end{equation}
\subsection{Inference Queries in PGMs}
Queries can have one of two types in the context of PGMS:\\ 

\noindent
\textbf{(1) Marginalization Queries:} Where the marginal distribution of a query variable is required given the observed variables, this is given by:
\begin{equation}
P(Q|O=o) = \frac{P(Q,O=o)}{P(O=o)}
\end{equation}
The joint distribution $P(Q, O=o)$ can be calculated by marginalization over the $H$ variables as follows:
\begin{equation}
P(Q,O=o) = \sum_{h \in val(H)} P(O=o, Q, H=h)
\end{equation}
While the prior can be deduced from marginalization over the $Q$ variables as follows:
\begin{equation}
P(O=o) = \sum_{q \in val(Q)} P(O=o, Q=q)
\end{equation}

\noindent
\textbf{(2) Maximum A Posteriori Queries:} Where the most likely instantiation of a variable is required given some observation, and is given by:
\begin{equation}
P^{*} = maximize(q) \; P(Q=q |O=o)
\end{equation}
Which is equivalent to the maximization of the joint distribution by marginalization over the variables of $H$. The above equation can then be rewritten to be:
\begin{equation}
P^{*} =  maximize(q) \; \sum_{h \in val(H)} P(Q=q, O=o, H=h)
\end{equation}

\subsection{Exact vs. Approximate Inference}
The compromise between the accuracy and efficiency of the inference step is determined based on the structure/complexity of the network and on the task itself. For instance, in real-time applications, with a loose need for best accuracy, approximate inference can be used. In some cases, exact inference is not even feasible with an increased number of variables and a complex dependencies. Variable elimination is one example of how an exact inference can be done. It replies on defining factors that contain multiple variable, then marginalizing over variables inside factors so inference results can be deduced. For approximate inference, variational methods are stated in some of the literature with not much of an illustration as the main method for approximate inference.
% ==========================================================================
\section{Applications of PGMs}
PGMs are used to model uncertainty in systems and to represent conditional dependency between random variables. The application at which such a scheme can be used are countless. For instance, they can be used in speech recognition, computer vision, or expert systems. Focusing on expert systems, an expert system is defined as a system that contains a knowledge base and an inference engine, where the inference can be done based on the knowledge base. The knowledge base can evolve over time and the inference engine must take that into consideration when producing new information. BNs and more generally PGMs are used to model uncertainty in such systems. Expert systems are very popular in the medical field where diagnosis assistance is needed. For instance, inferring the probability of a certain disease given a observable symptoms is one form of such systems. Other areas, such as insurance, stock market, among many other utilize such systems for the sake of modeling uncertainty due to its efficiency and applicability. 
% ==========================================================================
\section{Discussion}
\subsection{PGMs Pros and Cons}
As outlined multiple times during the course of this report, the pros of PGMs are mainly in the efficient representation and the utilization of the conditional independence for inference. Also, the applicability for scalability that PGMs have due to the same point of efficiency. However, during my reading in the materials available, I couldn't find any discussion of the memory requirements for maintaining a huge model, because this can be a bottleneck in some application where such availability of memory is not valid. Another point to mention is related to the learning process in BNs, specifically in the part where we need to learn the structure of the network by observable data samples. In this process, and due to the heuristic search approach adopted in the space of difference structures, the optimality of the solution is not guaranteed which means that the resulting structure may not the optimal one that can maximize $P(D|h)$.
\subsection{PGMs vs. Deep Learning}
Due to the network-like structure of both, the differences between them needs to be highlighted in order to get more understanding for the applicability of using any of them given a certain task. I believe the main difference is the observability of internal variables and nodes in PGMs which is not valid in case of neural networks and deep learning as all layers except for the input and output layers are considered hidden layers. However, deep learning have the ability to memorize data and relations much better than PGMs due to its structure that does not depend on random variables available in the system only. Also, the power to change internal structure based on the data in NNs gives it another advantage over PGMs where the structure can be pre-defined by domain experts. 
% ==========================================================================
\section*{Conclusion}
In this report, PGMs were discussed in details. The report started by providing detailed description of PGMs representation in the case of Bayesian Networks which represents random variables and the dependency relations among them in the form of acyclic directed graph (DAG). Then, learning PGMs was discussed as well, where we presented the three cases where learning is required which are: having both the structure and the conditional probability table as known information, or having only the structure, or having neither the structure nor the conditional probability tables. Then, the derivation of a gradient ascent algorithm for determining the conditional probability tables in case of known network structure was provided. After that, concepts for exact and approximate inference was discussed briefly. Finally, the report concluded by discussing potential applications of PGMs, the pros and cons of PGMs, and the similarities and differences between PGMs and deep learning models.
% ==========================================================================
\nocite{cs229}
\nocite{mitchell1997machine}
\nocite{koller2009probabilistic}
\nocite{pernkopf2014introduction}
\nocite{bishop2006pattern}
\nocite{kimArticle}
\nocite{beretta2018learning}
\bibliographystyle{ieeetr}
\bibliography{refs}
% ==========================================================================
\end{document}